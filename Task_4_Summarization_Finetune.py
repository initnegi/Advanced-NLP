# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wotyg4Yv6_zMM6vETXp1ipT0KhhXHhhk
"""

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from unsloth import FastLanguageModel, is_bfloat16_supported
from transformers import TrainingArguments
from trl import SFTTrainer
import torch

# 1. Load Dataset
articles_folder = '/content/drive/MyDrive/Advanced NLP/Dataset/business_articles'
summaries_folder = '/content/drive/MyDrive/Advanced NLP/Dataset/business_summaries'

def load_text_files(folder):
    texts = []
    filenames = sorted(os.listdir(folder))
    for filename in filenames:
        file_path = os.path.join(folder, filename)
        with open(file_path, 'r', encoding='utf-8') as file:
            texts.append(file.read().strip())
    return texts

articles = load_text_files(articles_folder)
summaries = load_text_files(summaries_folder)

df = pd.DataFrame({
    'article': articles,
    'human_summary': summaries
})

# 2. Format Dataset for Instruction Tuning
instruction_prompt = (
    "Given an article delimited by triple quotes, generate a concise summary of the key points from the article. "
    "Answer with the summary without any explanation."
)

def format_dataset(data):
    df = data.copy()
    def process_row(row):
        full_text = row['article']
        summary = row['human_summary']
        return pd.Series({
            "instruction": instruction_prompt,
            "input": full_text,
            "output": summary
        })
    df = df.apply(process_row, axis=1)
    return df

df = format_dataset(df)

# 3. Train/Test Split
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df = train_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

# 4. Load Model with Unsloth
max_seq_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Meta-Llama-3.1-8B-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

# 5. Apply LoRA/PEFT
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

# 6. Prepare Dataset for Training
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}

train_dataset = Dataset.from_pandas(train_df)
train_dataset = train_dataset.map(formatting_prompts_func, batched=True)

# 7. Training Arguments and Trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=250,
        learning_rate=2e-4,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=100,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
    ),
)

# 8. Train Model
trainer.train()

# 9. Save Model to Hugging Face Hub (replace with your actual HF repo and token)
# model.push_to_hub("your_hf_username/llama3.1_8b_text_summarization", token="hf_...")
# tokenizer.push_to_hub("your_hf_username/llama3.1_8b_text_summarization", token="hf_...")

# 10. Inference on Test Set
FastLanguageModel.for_inference(model)  # Enable fast inference

test_df['predictions'] = ""
for idx, row in test_df.iterrows():
    input_text = row["input"]
    prompt = alpaca_prompt.format(instruction_prompt, input_text, "")
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)
    decoded = tokenizer.batch_decode(outputs)
    # Extract only the summary part (after "### Response:")
    if "### Response:" in decoded[0]:
        summary = decoded[0].split("### Response:")[-1].strip()
    else:
        summary = decoded[0]
    test_df.at[idx, 'predictions'] = summary

# 11. Save Results
test_df.to_csv("summarization_test_results.csv", index=False)

print("Inference complete. Results saved to summarization_test_results.csv")