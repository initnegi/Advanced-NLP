# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wotyg4Yv6_zMM6vETXp1ipT0KhhXHhhk
"""

# sentiment_analysis_pipeline.py

import os
import re
import random
import pandas as pd
import numpy as np
from collections import Counter

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

import torch
from torch.utils.data import Dataset

from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

from transformers import (
    RobertaTokenizerFast,
    RobertaForSequenceClassification,
    Trainer,
    TrainingArguments
)

# Optional: For plotting (comment out if running headless)
import plotly.graph_objects as go

# 1. Load Data
DATA_PATH = "/content/drive/My Drive/Advanced NLP/Dataset/twitter_dataset"  # Change as needed
df = pd.read_csv(DATA_PATH, encoding="ISO-8859-1", names=["target", "ids", "date", "flag", "user", "text"])

# 2. Map sentiment labels
decode_map = {0: "NEGATIVE", 2: "NEUTRAL", 4: "POSITIVE"}
df['target'] = df['target'].apply(lambda x: decode_map[int(x)])

# 3. Remove unnecessary columns
df = df[["text", "target"]]

# 4. Filter sustainability-related tweets
sustainability_keywords = [
    'climate change', 'renewable energy', 'clean energy', 'sustainable', 'green energy',
    'carbon emissions', 'environment', 'recycling', 'solar power', 'wind energy', 'sustainability',
    'biofuel', 'global warming', 'sustainable transport', 'fossil fuels', 'net zero', 'greenhouse gases',
    'carbon footprint', 'conservation', 'pollution'
]

def contains_sustainability_keywords(text):
    text = str(text).lower()
    return any(keyword in text for keyword in sustainability_keywords)

testing_df = df[df['text'].apply(contains_sustainability_keywords)]
df = df[~df['text'].apply(contains_sustainability_keywords)]

# 5. Balance and sample training data
positive_samples = df[df['target'] == 'POSITIVE'].sample(n=15000, random_state=42)
negative_samples = df[df['target'] == 'NEGATIVE'].sample(n=15000, random_state=42)
training_df = pd.concat([positive_samples, negative_samples]).sample(frac=1, random_state=42).reset_index(drop=True)

# 6. Text cleaning
TEXT_CLEANING_RE = r"@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
stop_words = set(stopwords.words("english"))

def preprocess(text, stem=False):
    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()
    tokens = [token for token in text.split() if token not in stop_words]
    return " ".join(tokens)

training_df['text'] = training_df['text'].apply(preprocess)
testing_df['text'] = testing_df['text'].apply(preprocess)

# 7. Prepare data for modeling
label_binarizer = LabelBinarizer()
train_labels = label_binarizer.fit_transform(training_df['target']).astype('float32')
test_labels = label_binarizer.transform(testing_df['target']).astype('float32')
train_texts = training_df['text'].tolist()
test_texts = testing_df['text'].tolist()

# 8. Tokenizer and Model
tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=1)

# 9. Custom Dataset
class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = torch.tensor(self.labels[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': label
        }

train_dataset = CustomDataset(train_texts, train_labels, tokenizer)
val_dataset = CustomDataset(test_texts, test_labels, tokenizer)

# 10. Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = (logits > 0.5).astype(int)
    precision = precision_score(labels, predictions, average='binary', zero_division=0)
    recall = recall_score(labels, predictions, average='binary', zero_division=0)
    f1 = f1_score(labels, predictions, average='binary', zero_division=0)
    accuracy = accuracy_score(labels, predictions)
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

# 11. Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=15,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    learning_rate=1e-5,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=100,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=1000
)

# 12. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

# 13. Train
trainer.train()

# 14. Evaluate
eval_results = trainer.evaluate()
print(eval_results)

# 15. Save Model
MODEL_SAVE_PATH = '/content/drive/MyDrive/Advanced NLP/Models'
trainer.save_model(MODEL_SAVE_PATH)

# 16. Inference Example
def predict_sentiment(text, model_path=MODEL_SAVE_PATH, threshold=0.65):
    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')
    model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1)
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    sentiment_score = torch.sigmoid(logits).item()
    print(f"Sentiment score: {sentiment_score:.4f}")
    if sentiment_score > threshold:
        print("Positive Sentiment")
    else:
        print("Negative Sentiment")

# Example usage
predict_sentiment("I love this product!")

# 17. (Optional) Plotting
def plot_distribution(data, title="Sentiments distribution"):
    target_cnt = Counter(data)
    fig = go.Figure(data=[
        go.Bar(x=list(target_cnt.keys()), y=list(target_cnt.values()))
    ])
    fig.update_layout(
        title=title,
        xaxis_title="Sentiments",
        yaxis_title="Count",
        width=800,
        height=600
    )
    fig.show()

# plot_distribution(training_df['target'], "Training Sentiment Distribution")
# plot_distribution(testing_df['target'], "Testing Sentiment Distribution")